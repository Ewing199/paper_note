<attachment contentEditable="false" data-atts="%5B%5D" data-aid=".atts-e6117fb0-f64d-4157-8ac8-5abcbf3a8c49"></attachment>

[1]
[10]
[7]
[8]
[9]
[18]
[2]
[19-23]
[24]
[25]

[15]Yi J, Wen Z, Tao J, et al. CTC Regularized Model Adaptation for Improving LSTM RNN Based Multi-Accent Mandarin Speech Recognition[J]. Journal of Signal Processing Systems, 2018, 90(7): 985-997.
[16]Wang Z, Schultz T, Waibel A. Comparison of acoustic model adaptation techniques on non-native speech[C]//2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP'03). IEEE, 2003, 1: I-I.
[17]Liu Y, Fung P. Multi-accent Chinese speech recognition[C]//Ninth International Conference on Spoken Language Processing. 2006.
[18]Fung P, Liu Y. Effects and modeling of phonetic and acoustic confusions in accented speech[J]. the Journal of the Acoustical Society of America, 2005, 118(5): 3279-3293.
[19] Leading Group Office of Survey of Language Use in China (2006).
In survey of language use in China. Beijing: Yu Wen Press (in
Chinese).
[20]Davis S. B., & Mermelstein, P. (2013) Reliable Accent-Specific
Unit Generation With Discriminative Dynamic Gaussian Mixture
Selection for Multi-Accent Chinese Speech Recognition. IEEE
Trans Acoustics Speech Signal Process, 21 (10), 2073–2084
[21]Vergyri, D., Lamel, L., & Gauvain, L. (2010). Automatic Speech
Recognition of Multiple Accented English Data. In the Proceedings
of Interspeech
[10][22] Ding, G. H. (2008). Phonetic Confusion Analysis and Robust
Phone Set Generation for Shanghai-Accented Mandarin Speech
Recognition. In the Proceedings of Interspeech.
[11][23]Fosler-Lussier, E., Amdal, I., & Kuo, H.-K. J. (2005). A Framework
for Predicting Speech Recognition Errors. Speech Communication,
46(2), 153–170.
[12][24]Fosler-Lussier, E. (1999). Dynamic Pronunciation Models for
Automatic Speech Recognition. Ph.D. dissertation, Int. Comput.
Sci. Inst., Berkeley, CA, USA.
[13][25] Hain, T., & Woodland, P. C. (1999). Dynamic HMM Selection for
Continuous Speech Recognition. In Proc. Eurospeech, pp. 1327–
1330.
[14][26]V. Fisher et al. (1998). Speaker-Independent Upfront Dialect
Adaptation in A Large Vocabulary Continuous Speech
Recognition. In Proc. Int. Conf. Spoken Lang. Process.

[15][27]Wang, Z., Schultz, T., & Waibel, A. (2003). Comparison of
Acoustic Model Adaptation Techniques on Non-Native Speech.
In ICASSP 2003. IEEE, pp. 540–543.
[23][28]Huang, Y., Yu, D., Liu, C. J., & Gong, Y. F. (2014). Multi-Accent
Deep Neural Network Acoustic Model with Accent-Specific Top
Layer Using the KLD-Regularized Model Adaptation. In the
Proceedings of Interspeech.
[24][29]Huang, J., Li, J., Yu, D., Deng, L., & Gong, Y. F. (2013). CrossLanguage
Knowledge Transfer Using Multilingual Deep Neural
Network With Shared Hidden Layers. In the Proceedings of the
2013 I.E. International Conference on Acoustics, Speech and
Signal Processing (ICASSP).
[25][30]Chen, M. M., Yang, Z. Y., Liang, J. Z., Li, Y. P., Liu, W. J. (2015).
Improving Deep Neural Networks Based Multi-Accent Mandarin
Speech Recognition Using I-Vectors and Accent-Specific Top layer.
In the Proceedings of Interspeech.
[26][31]Sak, H., Senior, A., & Beaufays, F. (2014). Long Short-Term
Memory Based Recurrent Neural Network Architectures for
Large Vocabulary Speech Recognition. In the Proceedings of
Interspeech.
[30][32] Yi, J., Ni, H., Wen, Z. H., & Tao, J. (2016). Improving BLSTM
RNN Based Mandarin Speech Recognition Using Accent
Dependent Bottleneck Features. Asia-Pacific Signal and
Information Processing Association Annual Summit and
Conference
[31][33] Graves, A., Fernandez, S., Gomez, F., & Schmidhuber, J. (2006).
Connectionist Temporal Classification: Labelling Unsegmented
Sequence Data with Recurrent Neural Networks. In ICML,
Pittsburgh, USA
[32][34]Graves, A., Mohamed, A., & Hinton, G. (2013). Speech
Recognition With Deep Recurrent Neural Networks. In the
Proceedings of the 2013 I.E. International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, pp.
6645–6649
[33][35] Graves, A., & Jaitly, N. (2014). Towards End-To-End Speech
Recognition with Recurrent Neural Networks. In Proceedings of
the 31st International Conference on Machine Learning (ICML-
14), pp. 1764–1772.
[34][36] Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen,
E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A., et al. (2014).
Deepspeech: Scaling up End-To-End Speech Recognition. arXiv
preprint arXiv:1412.5567.
[37][37] Yu, D., Yao, K., Su, H., Li, G., & Seide, F. (2013). KL-Divergence
Regularized Deep Neural Network Adaptation for Improved Large
Vocabulary Speech Recognition. In the Proceedings of the 2013 I.E.
International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
[38]Dumoulin V, Perez E, Schucher N, et al. Feature-wise transformations[J]. Distill, 2018, 3(7): e11.
[39]Yoo S, Song I, Bengio Y. A Highly Adaptive Acoustic Model for Accurate Multi-dialect Speech Recognition[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 5716-5720.

[27]Elfeky M, Bastani M, Velez X, et al. Towards acoustic model unification across dialects[C]//2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2016: 624-628.
[28]



然而，上述模型是混合模型。这些采用交叉熵损失函数(CE)训练基于DNN-HMM或RNN-HMM的声学模型。DNNs或RNNs用于将语音帧分类为与上下文相关的集群状态(即senones)。集群状态是随着GMM-HMM迭代的。