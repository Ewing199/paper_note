<attachment contentEditable="false" data-atts="%5B%5D" data-aid=".atts-a6157514-3734-4ddc-870c-53fb573658eb"></attachment>
# 论文泛读
提出问题-->解决方案-->创新点

## 1.基于深度学习的语音识别研究与应用---黎长江
应用场景：在线英语学习网站的口语评分系统
针对的问题：
解决方案：
达到效果：

## 2.语音关键词识别技术的研究---孙成立，博士论文
技术参考文章
### 1.基于垃圾模型的识别
实时性高，计算量小，速度快
提出问题：



## 3.广播电视语音识别现状与应用策略

## 4.Multi-Accent Chinese Speech Recognition（2006）
4.1 在标准语音决策树上使用一个辅助方言语音决策树
4.2 达到效果：吴语和广东话提升了识别
4.3 ==传统方案==是语音和声学模型上的变化，将语音单元扩展到特定的语音单元，造成问题，过多的语音集合争议发音会引起解码器的混淆，这种方法包括使用大量带重音的语音[2]对声学模型进行再训练;应用最大后验(MAP)或最大似然比(MLLR)自适应适应特定口音的特征[5,6];并利用判别训练对声学模型[7]进行细化
缺点：模型参数不可逆转的发生了变化，模型也失去了兼容其他口音的能力
4.4 本文方法：在汉语语音识别的基础上，利用不同的声场单元集合，进行声场模型重构，在不同的重音中，不同的重音变化由不同的重音单位来表示。通过对状态绑定三音子ASR系统的决策树合并，在状态级进行了声学模型重构。
**确定一个特定的Accent-units**
**构建辅助Accent-tree**（在我们的系统中，我们使用基于决策树的状态绑定来关联上下文的三音模型[9]。问题集的选择、决策树的大小和中心单元的选择是基于决策树状态绑定的关键问题）
**重建声学模型**（为每个特定的重音(如粤语或吴语)构造一组独立的辅助重音树。辅助重音树的结构、形状和大小对于每个重音都是独特的，即使在不同重音组之间共享中心单元时也是如此）
实验方法：**三种MFCC特征**，我们使用基于**HTK决策树**的状态绑定过程构建了12个包含5500个绑定状态的gaussiancomponent triphone模型，**个人构建吴语，广东话数据集**
4.5 读后：只是针对了吴语广东话建模，重点在于根据**口音构建决策树**方法有效，可以尝试查找源码和实现

	
## 5.Multi-Accent Deep Neural Network Acoustic Model with Accent-Specific Top Layer Using the KLD-Regularized Model Adaptation
5.1 提出了一种多口音的深层神经网络声学模型，该模型具有特殊的顶层和共享的底层隐藏层。特定于口音的顶层用于模拟不同口音的特定模式。共享的底层隐藏层允许在本机和重音模型之间最大限度地共享知识。在低于100k的语音识别上MMI sequence-level标准效率更高

5.2 传统方法：在此之前，重音语音ASR领域已经做了大量的工作，大致可以分为模型适应方法和词汇适应方法。**模型适应方法**通常比词汇适应方法更有效

5.3 本文方法：顶层用于模拟口音的独特语音模式，采用模型自适应技术，采用KL-divergence (KLD)正则化深度神经网络模型自适应方法训练针对具体客户的顶层
![title](https://i.loli.net/2019/05/29/5cee4ead571f568792.png)
**深度神经网络的顶层设计**
缺点：相对多语言神经网络的方法，数据量小，使用KLD-正则方法避免过拟合问题

5.4 KLD正则化模型自适应
查看文献
5.5 总结
采用顶层模型模拟口音，底层隐藏层与母语共享知识，这种方式由于他的计算效率很实用。然后实用KLD正则自适应模型适应顶层
**展望**：我们正在进行的研究课题包括基于神经元放电足迹的神经元选择性重音模型自适应、正则化权重可调模型自适应和正则化序列重音模型自适应。此外，我们正在研究另一种口音不变的深度学习方法，使用编码口音的额外神经网络输入来标准化语音的口音不同方面

## 6.MULTI-DIALECT SPEECH RECOGNITION WITH A SINGLE SEQUENCE-TO-SEQUENCE MODEL
6.1 序列到序列模型为构建语音识别系统提供了一个简单而优雅的解决方案，它折叠了一个典型系统的独立组件，即声学(AM)、发音(PM)和语言(LM)建模成一个单一的神经网络。本文提供了一个从Listen，attend和spell的序列到序列模型，探究用单一模型解决不同英语口音的识别问题。提出问题的思路：将所有方言数据汇聚到一个LAS系统中无法微调，所考虑使用方言的特定信息合并到一个模型中，通过在原始图形序列末尾插入方言符号来修改训练目标，并将方言信息的1-hot表示形式输入模型的各个层。（针对英语）
6.2 **现状**：不同类方言的差距大，针对某一种方言的系统不能推广到所有的方言模型中、不可避免地，多方言对ASR系统提出了挑战。如果每种方言都有足够的数据，通常的做法是独立对待每种方言[2,4,5]。或者，在方言资源稀缺的情况下，这些模型可以使用来自其他方言的数据进行增强[3,6]。通常的方法是定义一组通用的手机模型[7-9]，使用适当的参数共享[6]，并对其进行来自多种语言的数据训练，最终根据感兴趣的语言的数据进行调整[10-13]。[11, 14]开发了类似的神经网络模型，使用语言无关的特征提取和语言相关的语音分类器，进行训练.[12]进一步研究了一种以方言相关音素识别为次要任务的基于图元的多方言模型.基于高斯混合模型的[13]系统通常采用MLLR和MAP等自适应技术;但是对于基于神经网络的模型，通过对特定于方言的数据的持续训练来适应是有效的
6.3 本文方法：自适应方法[24,25]通常用于将特定于方言的信息整合到系统中。我们假设通过显式地向LAS模型提供方言信息，我们应该能够弥合方言独立模型和方言依赖模型之间的鸿沟。首先，我们通过在图形序列[26]中引入一个人工标记，在输出中使用方言信息。LAS模型既要学习字形预测，又要学习方言分类。其次，将方言信息作为输入向量输入系统。它既可以作为附加到每一层输入的额外信息向量，也可以作为集群自适应训练的权重系数[27]。该系统有几个吸引人的优点:1)简单:模型无需更改，只需添加更多的数据即可扩展到更多的方言;2)低资源方言的改进:在多方言系统中，大部分参数是所有方言隐式共享的，这就迫使模型在训练过程中跨方言泛化。
6.4 MULTI-DIALECT LAS MODEL：将方言信息作为输出目标、方言信息作为输入向量、方言信息作为聚类系数

## 7.A HIGHLY ADAPTIVE ACOUSTIC MODEL FOR ACCURATE MULTI-DIALECT SPEECH RECOGNITION
### 7.1 现状
虽然方言特有的声学模型在一般情况下表现良好，但当方言特有的数据很少且每种语言的方言数量都很大时，它们就不容易维护；处理多方言语音识别的有效方法是为每种方言训练一种方言专用调幅，尽管总的来说性能很好，但缺点是需要为每种方言维护单独的AM，这会增加操作成本。
这些方法的主要缺点是，它们都需要为给定的语言维护多个模型，特别是当为多种语言和方言提供语音识别服务时，维护成本较高。

### 7.2 提出的方法：
我们提出的AM是基于方言信息及其内部表示动态调整的，这使得调幅具有高度的自适应能力，可以同时处理多种方言；
> 方法一：为多种方言构建单一AM的一种简单方法是根据多种方言的混合数据训练模型（是一种四路但表现不佳）
方法二：其中一种方法是基于多任务学习，训练AM不仅预测音素，而且识别方言，还有其他方法为AM提供辅助输入，如i-vector[7]或方言信息，使其适应不同的方言
**方法三**：特征线性调制(FiLM)是近年来引入的一种自适应神经网络建模技术，通过应用基于辅助输入的特征智能变换，使神经网络能够有效地适应大量问题[11]中的多个信息源（最初用于图像）；然而，不像最初的特征线性调制只基于外部输入来适应神经网络，我们还使用从网络中提取的内部表示作为额外的条件信息

我们还提出了一种简单而有效的训练方法来处理看不见的方言。

### 7.3 多口音声学模型
#### 基本模型
本文所考虑的AMs均采用LSTM层和softmax层的单向递归神经网络
1. 我们对每一个LSTM层的输入都像[15]一样进行**批处理归一化**，并在每一个LSTM层之后添加一个**前向卷积层**[16]，为模型提供一些**未来的上下文**。称之为dialect-unaware AMs（不使用任何方言信息的模型）
2. 通过相应的方言的数据微调dialect-unaware AMs，我们得到了dialect-specific的AMs
3. 添加口音信息作为额外输出，口音信息表示为一个热向量，并与其他输入特性连接在一起成为dialect-aware AM.
#### 外部信息作用（方言信息）
FiLM结构能够在多方言模型中使用方言信息作为条件输入。
![title](https://i.loli.net/2019/06/07/5cfa6b8a0df7023624.png)
其中方言信息d代表D维热向量,γ和β（FiLM公式）对于所有层都可以立即生成
![title](https://i.loli.net/2019/06/07/5cfa6c593327496558.png)
#### 内在信息作用
[4]表明，基于各层隐藏表示的AM动态自适应算法在说话人自适应中表现良好。该方法也可用于多方言语音识别


#### 内外信息共同作用
最后，我们提出在参数生成中不仅要使用方言信息，还要使用语音摘要。
#### 看不见的口音处理
我们的模型的一个限制是，用户总是需要提供方言信息。因此，即使用户在培训过程中使用看不见的方言，也必须选择一种方言(通常是母语)，这可能会导致性能下降。使用方言分类器可能是一种解决方案[5,6]，但它使总体性能在很大程度上依赖于分类的准确性
**在本文中，我们通过简单地增加一种称为未知的方言来处理看不见的方言问题**
在训练过程中，以一定概率随机抽取的话语(如0.1)被当作是来自未知方言。在测试时，使用看不见的方言的用户被认为使用了未知的方言，并根据这些方言生成条件表示

### 7.4总结展望
在本文中，我们提出了一种高度自适应的多方言语音识别方法。与之前的工作不同，**我们的AM既依赖于方言信息，也依赖于内部表示。我们还提出了一种简单而有效的方法来处理看不见的方言**。
实验结果表明，本文提出的方法是可行的。AM是一种维护成本较低的单模型，在多方言语音识别方面优于以往所有的AMs。
作为未来的工作，我们计划将我们提出的高度自适应声学建模技术应用于端到端语音识别。
由于端到端语音识别模型同时包含声学模型和语言模型，我们希望每个部分都能从我们提出的技术中受益	

## 8.Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin（2015）
### 8.1 由于端到端学习用神经网络取代了整个手工构建的组件管道，因此它允许我们处理各种各样的语音，包括嘈杂的环境、口音和不同的语言。
### 8.2研究现状
**端到端语音识别**是一个活跃的研究领域，当它被用来重新打分一个DNN-HMM(2014)的输出时或独立类(2014）时，会显示出令人信服的结果。有监督的RNN编解码器在预测音素 (2015)和音节 (2015)方面表现良好，CTC损失函数结合RNN对时间信息进行建模，在具有字符输出的端到端语音识别中也表现良好。CTC-RNN模型在预测音素方面也很有效(2015)，但在这种情况下仍然需要词典。
在单个**GPU**上进行培训可以获得显著的性能提升，随后线性扩展为两个GPU 或多个GPU 。也有利用提高单独GPU的效率用于低层次的基础深度学习的工作。还有在模型并行，数据并行或结合两者建立一个快速、高扩展性的语音识别系统来训练深度RNNs。
还有部分研究是优化**数据**的，数据增强在提高计算机视觉深度学习性能方面和语音识别方面，是非常有效的。例如，现有的语音引擎可以用来对齐和过滤数千小时的有声读物。
### 8.3模型架构
![title](https://i.loli.net/2019/06/07/5cfa0f4ddad7671811.png)
系统是递归神经网络(RNN)，它有一个或多个卷积输入层，然后是多个递归(单向或双向)层，在softmax层之前有一个完全连接的层。利用CTC损耗函数对网络进行端到端训练，它允许我们直接从输入音频预测字符序列。
网络的输入是一组功率归一化音频剪辑的对数谱图序列，在20ms windows下计算。输出是每种语言的字母表，CTC在每一个输出的时刻都会预测当前是需要输出一个字符还是空白符号
#### 8.3.1 Batch Normalization for Deep RNNs
为了在扩展训练集时有效地吸收数据，我们通过添加更多的重复层来增加网络的深度。然而，随着网络的规模和深度的增加，使用梯度下降训练网络变得更加具有挑战性。（PS：LSTM）本文使用**Batch Normalization**方法训练更深更快的网络。最近的研究表明，BatchNorm可以加快RNNs训练的收敛速度，但并不一定能提高泛化误差。
使用Batch Normalization的变体，只在卷积层的垂直方向上添加BN
#### 8.3.2 SortaGrad
> 原因：即使使用批处理规范化，我们发现使用CTC进行的训练有时也不稳定，尤其是在早期阶段。从零开始训练非常深的网络(或包含许多步骤的RNNs)可能会在早期的训练中失败，因为输出和梯度必须通过许多未调优的权重层传播。除了爆炸梯度(Pascanu et al.， 2012)， CTC通常会将接近零的概率分配给非常长的转录，使得梯度下降非常不稳定。
> 解决方法：SortaGrad:我们使用话语的长度作为难度的启发式，先训练较短(较容易)的话语
#### 8.4.3 Comparison of vanilla RNNs and GRUs
> 我们研究GRU（the Gated Recurrent Units）是因为在较小的数据集上的实验表明，在相同数量的参数下，GRU和LSTM达到了相似的精度，但是GRU训练更快，而且不太可能发散

> GRU和普通的RNN架构都从Batch Normalization受益
并利用深度网络显示出较强的性能
#### 8.4.4 Frequency Convolutions
>时域卷积是语音识别中常用的一种方法，它可以有效地模拟变长语音的时域平移不变性。与大型全连接网络相比，频率卷积试图更精确地模拟由于说话者可变性而引起的频谱方差.

#### 8.4.5 Lookahead Convolution and Unidirectional
Models
> 问题：双向RNN模型很难部署在一个在线的低延迟设置中，因为它们无法在用户发出声音时传输转录过程。
然而，只有正向递归的模型通常比类似的双向模型执行得更差，这意味着未来的上下文对于良好的性能非常重要

> 解决方案一：一种可能的解决方案是延迟系统发出标签，直到它有更多的上下文，但我们发现很难在我们的模型中诱导这种行为

> 为了建立一个没有任何精度损失的单向模型，我们开发了一个特殊的层，我们称之为前向卷积。该层学习权值，以线性组合每个神经元的激活τ步伐向前,从而允许我们控制所需的未来的上下文。我们把前向卷积放在所有循环层之上。这允许我们以更细的粒度流计算前向卷积下的所有计算。

#### 8.4.6 Adaptation to Mandarin
> 将传统的语音识别方法移植到另一种语言通常需要进行大量特定于语言的新开发。例如，经常需要人工设计发音模型，我们可能还需要显式地模仿语言的特定发音特征，比如普通话的声调

> 解决方案：由于我们的端到端系统直接预测字符，因此不再需要这些耗时的工作。我们对网络进行的唯一架构更改是由于字符集的特性

> 结论：我们在普通话中使用字符级语言模型，因为单词在文本中通常不分段。在第6.2节中，我们展示了我们的普通话语音模型在架构变化方面的改进大致与我们的英语语音模型相同，这表明从一种语言的开发中获得的建模知识可以很好地传递给其他语言
